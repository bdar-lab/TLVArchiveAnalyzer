# -*- coding: utf-8 -*-

"""
TLVArchiveAnalyzer
Data analysis tool designed to predict the true construction years of historical buildings based on a dataset of archived building documents.
This script uses categorized document types, historical data, and a predictive linear model to estimate missing construction years.

This script takes as input:
  1) A table of building document details generated by the TLVArchive project.
  2) A table of buildings with known construction years.
     Using this data, it groups document types into categories and assigns weights to each category
     based on their frequencies per period for estimating true construction years.

The script outputs a CSV file with predicted years for all buildings.

"""


import argparse
import datetime
from collections import defaultdict
import pandas as pd
import pyarrow.parquet as pq


# Define category lists by document types
CATEGORY1 = ["היתר-תכנית חתומה", "היתר מילולי חתום"]
CATEGORY2 = ["הודעת שומה על היטל השבחה,אגרות בניה,שוברי תשלום"]
CATEGORY3 = ["תעודת גמר", "תיק תעודת גמר", "טופס 4", "טופס 1"]
CATEGORIES = {"category1" :CATEGORY1,
              "category2": CATEGORY2,
              "category3": CATEGORY3}

# Defines the year ranges for analysis, each period will receive different category weights
PERIODS = {
    '1920-1925': range(1920, 1925+1),
    '1926-1943': range(1926, 1943+1),
    '1944-1985': range(1944, 1985+1),
    '1986-2021': range(1986, 2021+1),
}

# Sets min limit to avoid predicting too many years.
MIN_SCORE_PER_PERIOD = {
    '1920-1925':999,
    '1926-1943': 0.7,
    '1944-1985': 0.53,
    '1986-2021': 1.54,
}

# Fields to display in outputs
DISPLAY_COLUMNS = ['tik_id', 'ms_gush', 'ms_chelka', 'multiple_gush_chelka', 'address']

# Print and write to log file with time prefix
def log(msg):
    t = datetime.datetime.now().strftime("%d-%m-%Y %H:%M:%S")
    print(f"({t}) {msg}")


# Receives two csv files:
# One with true years of beginning constructions.
# Second with documents info.
# Merge the files based on tik_id, transform the data to a pivoted table where the types are fields.
# Export pivoted table into csv and parquet
def pivot_csvs(true_years_csv, documents_csv, output_csv, output_parquet):

    # Read true-years csv into a DataFrame
    log(f"Reading {true_years_csv}..")
    true_years_df = pd.read_csv(true_years_csv)

    # Read documents csv into a DataFrame
    log(f"Reading {documents_csv}..")
    docs_df = pd.read_csv(documents_csv)

    # Group documents info table by 'tik_id' and 'type' and aggregate 'date' into a list of years
    log(f"Grouping {documents_csv}..")
    docs_df['year'] = pd.to_datetime(docs_df['date'], format='%d/%m/%Y', errors='coerce').dt.year
    docs_grouped_df = docs_df.groupby(['tik_id', 'type'])['year'].apply(list).reset_index()

    # Pivot the grouped table to have one column per document 'type' with a list of dates
    log(f"Pivoting grouped {documents_csv}..")
    docs_pivoted_df = docs_grouped_df.pivot_table(index='tik_id', columns='type', values='year', aggfunc=lambda x:x).reset_index()

    # Merge true-years table and documents pivoted table by 'tik_id'
    log(f"Merging grouped data..")
    merged_df = pd.merge(docs_pivoted_df, true_years_df, on='tik_id', how='left')

    # Getting the fields of DISPLAY_COLUMNS (gush, chelka, address), unique for each tik_id
    docs_df_unique = docs_df.drop_duplicates(subset='tik_id', keep='first')[DISPLAY_COLUMNS]
    merged_df = pd.merge(merged_df, docs_df_unique, on='tik_id', how='left')

    # Export to csv
    log(f"Writing {output_csv}..")
    merged_df.to_csv(output_csv, index=False, encoding='utf-8-sig')

    # Export to parquet
    log(f"Writing {output_parquet}..")
    merged_df.to_parquet(output_parquet, engine='pyarrow', index=False)
    return merged_df


# Aggregates years between multiple records into list
def join_years(row, category) :
    years = []
    for doc_type in CATEGORIES[category]:
        value = row[doc_type]
        if value is not None:
            years += [int(y) for y in list(value)]
    return sorted(years)


# Add categories to table based on types
def add_categories(df):

    categories_df = df[DISPLAY_COLUMNS + ["true_year"]].copy()
    for category in CATEGORIES:
        categories_df[category] = df.apply(lambda x: join_years(x, category) , axis=1)
    return categories_df

# Calculates weights for each category according to documents years frequency, using linear model.
def get_period_weights(df):

    # Initialize dictionaries to store appearance counts per period
    category_counts_per_period = {period: defaultdict(int) for period in PERIODS}
    gap_counts_per_period = {period: defaultdict(int) for period in PERIODS}

    # Keep only records with true_year
    df_true_year = df.dropna(subset=['true_year'])

    # Analyze category for each year range
    for _, row in df_true_year.iterrows():
        true_year = row['true_year']
        true_year = int(true_year)
        period = get_period(true_year)

        # Ignore true years out of range
        if not period:
            continue

        # Count years for each period and for each category
        # Count years-within-3-year-gap for each period and for each category
        for category in CATEGORIES:
            years = row[category]
            if not years:
                continue
            category_counts_per_period[period][category] += len(years)
            years_within_3_gap = [y for y in years if abs(y - true_year) <= 3]
            gap_counts_per_period[period][category] += len(years_within_3_gap)

    # Calculate predictive power for each period
    weights_per_period = {}
    for period in PERIODS:
        weights_per_category = []
        for category in CATEGORIES:
            category_count = category_counts_per_period[period][category]
            weight = 0
            if category_count > 0:
                gap_count = gap_counts_per_period[period][category]
                weight = gap_count / category_count
            weights_per_category.append((category, weight))
        weights_per_period[period] = weights_per_category

    return weights_per_period


# Find the period of the year
def get_period(year):
    for period, period_range in PERIODS.items():
        if year in period_range:
            return period
    return None


# Calculate the scores based on weights, that were calculated using years frequencies
def get_scores(row, period_weights):
    year_scores = defaultdict(float)
    filtered_years = {}
    for category in CATEGORIES:
        filtered_years[category] = []
        for year in row[category]:
            period = get_period(year)
            if period:
                weights = period_weights[period]
                weight = 0
                for c, weight in weights:
                    if c == category:
                        break
                filtered_years[category].append(year)
                year_scores[year] += weight

    for category in CATEGORIES:
        row[category] = filtered_years[category]
        row[f"{category}_scores"] = [year_scores[y] for y in filtered_years[category]]
    return row

# Calculate scores per year per category
# based of weights per category per period
def get_all_scores(pivoted_df, period_weights):
    results = []
    for _, row in pivoted_df.iterrows():
        record = {col: row[col] for col in pivoted_df.columns}
        get_scores(record, period_weights)
        results.append(record)
    scores_df = pd.DataFrame(results)
    return scores_df

# Check 10-years gap
def is_gapped(year, selected_years):
    for selected_year, _ in selected_years:
        selected_period = get_period(selected_year)
        if selected_period!='1920-1925' and abs(year - selected_year) <= 10:
            return False
    return True

# Check gap
def is_exists_within_range(this_year, other_years, gap):
    for other_year in other_years:
        if abs(this_year-other_year) <= gap:
            return True
    return False


# Predict the true-year
# Take the years with the highest scores with the following restrictions:
#   (1) Must be higher than the min
#   (2) Must have 10-years gap between
#   (3) For years in range 1920-1925, select only one and only if found doc in category1 in that year
#   (4) If year from period 1920-1925 was selected, ignore years 1926-1930.
#   (5) If year from range 1944-2021 was selected, ignor it if can't find doc in category1/3 within 3 years.
#   (6) For years before 1980, select max 2 years, and for others max 4 years.
def predict(row):
    years = []
    scores = []
    for category in CATEGORIES:
        years += row[category]
        scores += row[f"{category}_scores"]
    years_scores = list(zip(years, scores))

    # Sort all year according to their scores
    years_scores.sort(key=lambda x: x[1], reverse=True)

    # Handle period 1920-1925
    selected_years = []
    period_1920_1925_selected = False

    # Handle period 1920-1925, pick only one and only if found doc in category1
    for year, score in years_scores:
        period = get_period(year)
        if period == '1920-1925' and not period_1920_1925_selected and year in row['category1']:
            selected_years.append((year, score))
            period_1920_1925_selected = True
            break

    # Loop for the rest periods
    for year, score in years_scores:
        period = get_period(year)

        # Remove years with score less than min
        min_score = MIN_SCORE_PER_PERIOD[period]
        if score <= min_score:
            continue
        # Special case - year from period 1920-1925 was selected, ignoring years 1926-1930
        if period_1920_1925_selected and year in range(1926, 1930+1):
            continue

        # Remove years from 1944-2021 if not found docs in category1/3 within 4 years
        if period in ["1944-1985", "1986-2021"]:
            valid1 = is_exists_within_range(year, row["category1"], 4)
            valid2 = is_exists_within_range(year, row["category3"], 4)
            if not valid1 and not valid2:
                continue

        # Force 10-years gap
        if is_gapped(year, selected_years):
            selected_years.append((year, score))

    # Sort and update record
    # For years >= 1980 Take only 4 meaningful years
    # For years < 1980 take only 2 meaningful years
    selected_years.sort(key=lambda x: x[0], reverse=False)
    idx = 1
    for year, score in selected_years:
        if (year < 1980 and idx <= 2) or (year>=1980 and idx <= 4):
            row[f"predicted_year{idx}"] = year
            row[f"score{idx}"] = score
            idx += 1


# Predict true year of all records
def predict_all(scored_df):
    results = []
    for _, row in scored_df.iterrows():
        record = {col: row[col] for col in scored_df.columns}
        predict(record)
        results.append(record)

    predicted_df = pd.DataFrame(results)
    return predicted_df

# Group tik_id, split true_year to two columns
def reshape(results_df):
    # Get the other columns besides 'tik_id' and 'true_year'
    other_columns = [col for col in results_df.columns if col not in ['tik_id', 'true_year']]

    # Group by 'tik_id' and apply aggregation for 'true_year' and other columns
    df_grouped = results_df.groupby('tik_id', as_index=False).agg(
        {**{col: 'first' for col in other_columns}, 'true_year': lambda x: list(x)}
    )

    # Expand 'true_year' into separate columns (true_year1, true_year2, ...)
    true_years_df = df_grouped['true_year'].apply(pd.Series)  # Expands lists to separate columns

    # Merge the 'true_year' columns back with the original DataFrame
    results_df = df_grouped.drop('true_year', axis=1).join(true_years_df.add_prefix('true_year'))

    return results_df


def main():

    # Creates the argument parser
    parser = argparse.ArgumentParser(description="TLVArchiveAnalyzer")

    # Adds arguments to the parser
    parser.add_argument("--true_years", default="years.csv", help='path to the true years csv')
    parser.add_argument("--docs", default=r'orig\all_docs.csv', help='path to the documents data csv')
    parser.add_argument("--export", action='store_true', help='extract csvs to parquet')
    parser.add_argument("--print_scores", action='store_true', help='Print the scores')
    args = parser.parse_args()

    # Uses parquet file to parse big data file very fast
    parquet_data = "pivoted_data.parquet"

    # Exports the csv files into reduced csv and parquet file to allow fast loading
    if args.export:
        output_csv = "pivoted_data.csv"
        log(f"exporting to {output_csv} and {parquet_data}")
        pivot_csvs(args.true_years, args.docs, output_csv, parquet_data)
        exit()

    # Loads the parquet file
    log(f"Reading data.parquet..")
    loaded_table = pq.read_table(parquet_data)
    pivoted_df = loaded_table.to_pandas()

    log("Adding categories..")
    categorized_df = add_categories(pivoted_df)

    # Calculates the weights for each period for each category/type frequency.
    log(f"Getting weights..")
    period_weights = get_period_weights(categorized_df)

    log("Calculating scores..")
    scores_df = get_all_scores(categorized_df, period_weights)

    # Prints all scores for each category/type for each year
    if args.print_scores:
        log(f"Exporting all scores..")
        scores_df.to_csv("all_scores.csv", index=False, encoding='utf-8-sig')

    # Predicts the true-year based on calculated weights
    log(f"Predicting..")
    predicted_df = predict_all(scores_df)

    # Group tik_id
    log("Reshaping..")
    results_df = reshape(predicted_df)

    # Exporting output to csv
    output_csv = "predicted_years.csv"
    log(f"Writing {output_csv}..")
    results_df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    log("DONE")


# Run the script
if __name__ == "__main__":
    main()
